{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opensearch-py pyspark matplotlib scikit-learn seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "rootpath = os.path.abspath('/home/robertc/Git/pfun-cma-model')\n",
    "if rootpath not in sys.path:\n",
    "    sys.path.insert(0, rootpath)\n",
    "from pfun_cma_model.embed import EmbedClient\n",
    "os = EmbedClient(require_ssh_tunnel=False).opensearch_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = os.search(index=\"embeddings\", body={\"size\": 5000, \"_source\": \"embedding\"}, scroll='1m')\n",
    "scroll_id = res['_scroll_id']\n",
    "scroll_size = res['hits']['total']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [(d['_source']['embedding'][0]['embedding'],)\n",
    "              for d in res['hits']['hits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 20:05:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"k8s://https://192.168.1.64:16443\") \\\n",
    "    .config(\"spark.driver.host\", \"192.168.1.64\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"apache/spark:latest\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.jars\", os.path.join(rootpath, \"embed/embed/pyspark_jars/elasticsearch-hadoop-8.10.2.jar\")) \\\n",
    "    .appName(\"pfun-cma-model-embed\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromOSWithSpark(index: str = \"embeddings\", sample_fraction: float = 0.1):\n",
    "    #: Get data from opensearch (with spark)\n",
    "    df = (\n",
    "        spark.read.format(\"org.elasticsearch.spark.sql\")\n",
    "        .option(\"es.nodes.wan.only\", \"true\")\n",
    "        .option(\"es.port\", \"9201\")\n",
    "        .option(\"es.net.ssl\", \"true\")\n",
    "        .option(\"es.nodes\", \"192.168.1.64\")\n",
    "        .load(f\"{index}/doc_type\")\n",
    "    )\n",
    "\n",
    "    # Create random sample of 10% of the data\n",
    "    df_sample = df.sample(False, sample_fraction)\n",
    "\n",
    "    return df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType, StructType, StructField\n",
    "\n",
    "schema = StructType([StructField(\"list_features\", ArrayType(DoubleType()))])\n",
    "df = spark.createDataFrame(embeddings, schema=schema)\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to convert array into vector\n",
    "vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df = df.withColumn(\"features\", vector_udf(\"list_features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition('features')\n",
    "df.persist()\n",
    "spark.conf.set('spark.sql.shuffle.partitions', int(16 * 2.5))\n",
    "spark.conf.set('spark.default.parallelism', 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 20:06:17 WARN TaskSetManager: Stage 0 contains a task of very large size (13841 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 3) / 3]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=8, seed=23)\n",
    "model = kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "df_pandas = pd.DataFrame(model.transform(df).rdd.map(lambda r: (float(r.features[0]), float(r.features[1]), int(r.prediction))).collect(), columns=[\"x\", \"y\", \"cluster\"])\n",
    "df_pandas['x'], df_pandas['y'] = zip(*pca.fit_transform(df_pandas[[\"x\", \"y\"]]))\n",
    "plt.rc('figure', figsize=(10, 8))\n",
    "sns.scatterplot(x='x', y='y', hue='cluster', data=df_pandas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10+"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
